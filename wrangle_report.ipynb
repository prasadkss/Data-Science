{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this paper we will describe our wrangling effort made in the section of wrangling weRateDog project\n",
    "\n",
    "Data wrangling consists of:\n",
    "\n",
    "-  Gathering data\n",
    "-  Assessing data\n",
    "-  Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "We need to gather 3 datasets in this project. I have named them as below per my convenience.\n",
    "\n",
    "- WeRateDogs\n",
    "- TweetImage\n",
    "- Tweets\n",
    "\n",
    "The WeRateDogs dataframe is gathered directly from the existing csv file twitter-archive-enhanced.csv <br>\n",
    "The TweetImage dataframs is downloaded programmatically from the Udacity's servers <br>\n",
    "The Tweets dataframs is gathered from querying Twitter using Tweepy for the retweets and favorates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessing Data\n",
    "\n",
    "Data accessiung can be done in two ways.\n",
    "-  Programatically\n",
    "-  Manually\n",
    "\n",
    "I have used the python libraries to figure out the Quality and Tidiness issues in the provided dataset, instead of doing manually. We could detect and document the following quality issues and tidiness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality\n",
    "\n",
    "##### `WeRateDogs` dataset\n",
    "- `name` is sometimes not an actual name\n",
    "- wrong data types (`in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `timestamp` and `retweeted_status_timestamp`)\n",
    "- missing some `expanded_urls`\n",
    "- 343rd entry is not a dog rating\n",
    "- some entries should be classified as puppers (missing data)\n",
    "- some entries are retweets\n",
    "- Extra characters after '&'\n",
    "\n",
    "##### `TweetImage` dataset\n",
    "- `p1`, `p2`, `p3` inconsistent capitalization (sometimes first letter is capital)\n",
    "- missing data (only has 2075 entries instead of 2356)\n",
    "\n",
    "##### `Tweets` dataset\n",
    "- missing data (only has 2345 entries instead of 2356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness\n",
    "\n",
    "- Three data frames `WeRateDogs`, `TweetImage`, and `tweets` should be one (combined table) since all tables' entries are each describing one tweet\n",
    "\n",
    "##### `WeRateDogs` dataset\n",
    "- one variable in four columns (`doggo`, `floofer`, `pupper`, and `puppo`)\n",
    "- We may want to add a gender column from the text columns in archives dataset\n",
    "- Get rid of image prediction columns\n",
    "- Delete unneccessary columns and rename few"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After accessing data and find out the the quality and tidiness issues programatically/maually, the next step is cleaning the issues we find out.\n",
    "\n",
    "The cleaning step also involves can be done by wither manually or programatically. If the issue is related to one record/column, which might not require and programming efforts as we can clean the issue manually. But if the issue is bigger than it looks and the dataset contains more number of records, then we better do programatic clean up.\n",
    "\n",
    "Our process was Define, Code and Test and we were always making a copy of tha dataset even we made the copy in file to test the change before applying to the main dataset. We didn't spot all the quality and tidiness assessments at the assessing data section, so we have been iterating and revisiting assessing to add these assessments to our notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling indeed is a core skill that everyone who works with data should be familiar with since so much of the world's data isn't clean. If we analyze, visualize, or model our data before we wrangle it, our consequences could be making mistakes, missing out on cool insights, and wasting time. We couldn't be able to make some of the visualization without wrangling (i.e dog gender partition) So best practices say wrangle. Always."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
